Accessing data in Microsoft Fabric using Python can be achieved by treating OneLake as Azure Data Lake Storage (ADLS) Gen2 and connecting to the Real-Time hub endpoints (e.g., Event Hubs, Kafka) using the relevant Azure SDKs. Data discovery often happens within the Fabric UI, which provides connection details for programmatic access.


// 1. Get Data from OneLake via Python 
OneLake is compatible with ADLS Gen2 APIs, allowing you to use existing Azure storage SDKs for Python. The primary method for authentication is using an Azure Entra ID token. 


Prerequisites:
- Install necessary Python packages: azure-identity, azure-storage-file-datalake, deltalake.
- Run az login in your terminal to authenticate via Azure CLI, which DefaultAzureCredential will use. 



from azure.identity import DefaultAzureCredential
from azure.storage.filedatalake import DataLakeServiceClient

# OneLake account name is always 'onelake'
account_name = "onelake"
# The URL for the DFS endpoint
dfs_url = f"https://{account_name}.dfs.fabric.microsoft.com"

# Acquire a token using DefaultAzureCredential (uses 'az login' credentials)
credential = DefaultAzureCredential()

# Create the DataLakeServiceClient
service_client = DataLakeServiceClient(account_url=dfs_url, credential=credential)

# Specify your Workspace name as the filesystem and include the item type in the path
# Format: "workspace_name/item_name.item_type/folder/file"
file_system_client = service_client.get_file_system_client(file_system="YourWorkspaceName")
paths = file_system_client.get_paths(path="YourLakehouseName.Lakehouse/Files/")

print("Listing directory contents:")
for path in paths:
    print(path.name)





2. Create a Data Connection 
Connections are generally managed within the Fabric portal UI. Programmatic access uses the details (endpoints, connection strings, authentication methods) generated by these Fabric items. 

- In the UI: Navigate to the "Manage connections and gateways" section in settings to create reusable connections for data pipelines.

- Programmatically: You use the specific connection details (like the SQL endpoint for a Warehouse or the Event Hub connection string for real-time data) directly in your Python code as shown in the examples above and below.  



3. Discover Data using OneLake Catalog and Real-Time hub:

- Data discovery primarily occurs within the OneLake catalog in the Fabric UI, which provides a searchable inventory of data assets. Programmatic discovery typically involves connecting to the specific service endpoint after identifying the item in the catalog. 

Real-Time Hub with Python (Event Hubs):

- For the Real-Time hub, you can consume events from sources like Azure Event Hubs using the azure-eventhub Python package. 


Prerequisites
- Install azure-eventhub package: pip install azure-eventhub.
- Obtain the connection string for the Event Hub behind your Fabric Eventstream from the Fabric portal. 






// 2. Connect to the Real-Time Hub using Python 
The Real-Time hub in Fabric manages event streams that can be accessed using standard protocols like Azure Event Hubs APIs or Kafka endpoints. The Python interaction typically involves sending or receiving data using the azure-eventhub library. 

# Prerequisites

Install the necessary Python package: 

pip install azure-eventhub

Python Example: Send events to a Fabric Eventstream 
This example shows how to act as a producer and send data to a custom source within a Fabric eventstream. 

import asyncio
from azure.eventhub import EventHubProducerClient, EventData
import time

# Get these values from your Fabric Eventstream custom source details (Kafka or Event Hubs protocol)
EVENT_HUB_CONN_STR = "<Your_Event_Hub_Connection_String>"
EVENT_HUB_NAME = "<Your_Event_Hub_Name>"

async def run():
    # Create a producer client to send events to the event hub
    producer = EventHubProducerClient.from_connection_string(
        conn_str=EVENT_HUB_CONN_STR, 
        eventhub_name=EVENT_HUB_NAME
    )
    async with producer:
        # Create a batch
        event_data_batch = await producer.create_batch()
        
        # Add events to the batch
        event_data_batch.add(EventData('{"event": "data point 1"}'))
        event_data_batch.add(EventData('{"event": "data point 2"}'))
        
        # Send the batch of events to the event hub
        await producer.send_batch(event_data_batch)
        print("Sent batch of events")

if __name__ == "__main__":
    # Ensure you have an async loop running
    asyncio.run(run())

To run the async code in a standard Python script, use asyncio.run(run()). Within a Fabric notebook, you would typically use an existing event loop or specific notebook features. 



// 3. Create a Data Connection Programmatically 

Creating a data connection in Fabric is typically a manual process through the UI in the "Manage connections and gateways" section. Programmatic creation is generally handled internally by Fabric services or via the Power BI REST APIs, which can be complex. 


For Python code within a User Data Function (UDF) item in Fabric, you can reference an existing, pre-configured connection using specific decorators: 


# Example of referencing a connection in a Fabric User Data Function (UDF)
from fabric.functions import udf
from fabric.functions.fabric_class import FabricSqlConnection

# Assumes a connection with alias "MySqlConn" was created in the UI
@udf.connection(alias="MySqlConn", argName="sql_connection")
@udf.function()
def my_function(sql_connection: FabricSqlConnection):
    # Use the connection within the function
    # ... code to execute query using sql_connection ...
    pass






OR











In Microsoft Fabric, you can interact with OneLake data using the Python Azure Data Lake Storage (ADLS) SDK and discover data in the Real-Time hub by creating specific data connection items within the Fabric portal. The Python interaction primarily uses the standard ADLS APIs and URI schemes. 

# Get Data and Create a Connection with OneLake (Python Example) 

- You can access data stored in OneLake from an external Python environment or a Fabric Notebook by treating OneLake as an Azure Data Lake Storage Gen2 (ADLS Gen2) account. 

1. Install necessary libraries:

pip install azure-storage-file-datalake azure-identity
NOTE: Use code with caution.

2. Authenticate: Use DefaultAzureCredential for seamless authentication if running in an environment with access to Azure identity, or use the az login command in Azure CLI.

3. Define OneLake URI: The format is abfss://<workspace>@onelake.dfs.fabric.microsoft.com/<item>.<itemtype>/<path>.

4. Python code to connect and list files:
import os
from azure.storage.filedatalake import DataLakeServiceClient
from azure.identity import DefaultAzureCredential

# Replace with your workspace and lakehouse names/GUIDs
workspace_name = "YourWorkspaceName"
lakehouse_name = "YourLakehouseName"

# Construct the ADLS Gen2 compatible URL
account_name = "onelake"
account_host = f"{account_name}.dfs.fabric.microsoft.com"
filesystem_name = f"{workspace_name}" # In ABFS, workspace is the filesystem
container_name = f"{lakehouse_name}.lakehouse" # Item type is part of the path in ABFS URI

# Authenticate
# DefaultAzureCredential automatically tries several authentication methods
credential = DefaultAzureCredential()

# Create DataLakeServiceClient
datalake_service_client = DataLakeServiceClient(account_url=f"https://{account_host}", credential=credential)

# Get a file system client for your lakehouse (workspace level)
try:
    file_system_client = datalake_service_client.get_file_system_client(filesystem=filesystem_name)
    # List contents of the 'Files' directory within the lakehouse
    # The container_name (item type) is used in the get_paths call
    paths = file_system_client.get_paths(path=container_name + "/Files")
    for path in paths:
        print(path.name)
except Exception as e:
    print(f"Error accessing OneLake: {e}")
Use code with caution.


This approach uses existing ADLS APIs to interact with OneLake as if it were a standard ADLS Gen2 storage
