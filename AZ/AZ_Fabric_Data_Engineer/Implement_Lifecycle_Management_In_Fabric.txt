Implementing lifecycle management in Microsoft Fabric primarily involves two aspects: Application Lifecycle Management (ALM) for continuous integration/continuous delivery (CI/CD), and data management policies for data retention and caching. Code examples in Python and KQL help manage data within these structures. 


//Application Lifecycle Management (ALM)
ALM in Fabric uses Git Integration and Deployment Pipelines to manage content (like notebooks, lakehouses, and reports) across development, test, and production stages. This provides version control and automated deployment. 

/// Example Steps: Set up a Deployment Pipeline
1. Create Workspaces: Set up separate workspaces for your development, test, and production environments.
2. Connect to Git: In your development workspace, configure Git integration via Workspace settings to an Azure DevOps repository for source control.
3. Create a Deployment Pipeline:

 - Navigate to the Fabric home page and select Deployment Pipelines from the left navigation pane.
 - Select New deployment pipeline, give it a name, and define the stages (e.g., Development, Test, Production).
4.  Assign Workspaces: Assign your corresponding workspaces to each stage in the pipeline.
5.  Develop and Commit: Create or modify items (e.g., a Spark notebook) in your development workspace. Commit these changes to your Git repository.
6.  Deploy: Deploy the content from the Development stage to the Test, and subsequently to Production stages, using the pipeline UI. This promotes the changes through the established lifecycle process. 

//  Code Example (Python in a Fabric Notebook for CI/CD)
While the deployment process itself is largely UI-driven or can be automated via Azure DevOps pipelines, you can use Python within a notebook to interact with the data plane or control the data flow as part of the pipeline steps. 


# Example: Python code in a notebook used for data transformation in a 'Test' stage
# This notebook is part of the ALM process managed by Git and Deployment Pipelines

# Import necessary libraries (e.g., pyspark for data processing)
from pyspark.sql import SparkSession

# Assumed variables for the Lakehouse in the current environment
lakehouse_name = "MyTestLakehouse"
table_name = "RawData"

# Create a Spark session
spark = SparkSession.builder.getOrCreate()

print(f"Reading data from {table_name} in {lakehouse_name} for testing...")

# Read data from the source table (assuming a shortcut/mount point exists)
# The actual path might look like 'Files/myfolder/mytable' or 'Tables/mytable' in the Lakehouse
df = spark.read.format("delta").load(f"abfss://<container>@<storage_account>.dfs.core.windows.net/{lakehouse_name}.Lakehouse/Tables/{table_name}")

# Apply transformation logic (example: filter out test data)
filtered_df = df.filter("category != 'test'")

# Write the transformed data to another location or back to the same lakehouse with a different table name
output_table_name = "CleanedData"
filtered_df.write.format("delta").mode("overwrite").save(f"abfss://<container>@<storage_account>.dfs.core.windows.net/{lakehouse_name}.Lakehouse/Tables/{output_table_name}")

print(f"Data transformed and saved to {output_table_name}.")



/// Data Management Policies:
Data lifecycle management at the storage level involves setting data retention and caching policies directly on KQL databases or by managing data in OneLake using Spark/Python.

// Example Steps: Define a Data Retention Policy (KQL)
You can set data retention policies using KQL management commands within a KQL Queryset or a Notebook. 

1. Open a KQL Queryset: In your Fabric workspace, open the KQL Queryset associated with your KQL database.
2. Run the Management Command: Use the .alter table command to define the retention policy. 

// Code Example (KQL)
This command sets a data retention policy for the MyDataTable table, retaining data for 30 days and ensuring it's permanently deleted after that period. 


.alter table MyDataTable policy retention softdelete = 30d recoverability = disabled


// Code Example (Python to manage data in Lakehouse):
For data in a Lakehouse (Delta tables), you typically use Spark within a notebook to manage the data lifecycle (e.g., archiving old data, vacuuming). 


# Example: Python code to vacuum a Delta table, cleaning up old versions
from delta.tables import DeltaTable

delta_table_path = "abfss://<container>@<storage_account>.dfs.core.windows.net/MyLakehouse.Lakehouse/Tables/HistoricalData"
deltaTable = DeltaTable.forPath(spark, delta_table_path)

# Vacuum the table, retaining data for a period of time (e.g., 72 hours/3 days)
# This physically removes data files no longer referenced by the table
deltaTable.vacuum(retentionHours=72)



