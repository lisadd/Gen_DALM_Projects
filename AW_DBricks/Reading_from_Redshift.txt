Ensure the Redshift JDBC Driver and Spark-Redshift Connector are Available:


Step1: Must provide connection details for your Redshift cluster and specify a temp S3 Bucket for data staging


A. Python Configs:
    from pyspark.sql import SparkSession

    spark = SparkSession.builder \
        .appName("RedshiftReadExample") \
        .config("spark.jars.packages", "io.github.spark-redshift-community:spark-redshift_2.12:4.0.1,org.postgresql:postgresql:42.2.5") \
        .getOrCreate()

    # Redshift connection details
    redshift_url = "jdbc:redshift://your-redshift-cluster-endpoint:5439/your-database"
    redshift_user = "your-redshift-username"
    redshift_password = "your-redshift-password"
    temp_s3_bucket = "s3a://your-temp-s3-bucket/"


Read Data from a Redshift Table (Python):

    df = spark.read \
        .format("io.github.spark_redshift_community.spark.redshift") \
        .option("url", redshift_url) \
        .option("user", redshift_user) \
        .option("password", redshift_password) \
        .option("dbtable", "your_redshift_table_name") \
        .option("tempdir", temp_s3_bucket) \
        .load()


   display(df)

Reading with a Custom Query(Python):

    custom_query = "(SELECT column1, column2 FROM your_redshift_table_name WHERE condition) AS custom_alias"

    df_query = spark.read \
        .format("io.github.spark_redshift_community.spark.redshift") \
        .option("url", redshift_url) \
        .option("user", redshift_user) \
        .option("password", redshift_password) \
        .option("query", custom_query) \
        .option("tempdir", temp_s3_bucket) \
        .load()


Work with the DataFrame(Panda - Python):
Once the data is loaded into a Spark DataFrame, you can perform various Spark operations (transformations, actions) on it:


    df.show()
    df.printSchema()


