/* Evaluate a Model - After training, evaluate the model's performance using appropriate metrics. */

from sklearn.metrics import classification_report, confusion_matrix

# Assuming 'model' is your trained model and X_test, y_test are available

# Make predictions on the test set
y_pred = model.predict(X_test)

# Print classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Print confusion matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# You can also log these evaluation results to MLflow
with mlflow.start_run(run_name="Model Evaluation", nested=True):
    mlflow.log_text(classification_report(y_test, y_pred), "classification_report.txt")
    mlflow.log_text(str(confusion_matrix(y_test, y_pred)), "confusion_matrix.txt")
    print("Evaluation metrics logged to MLflow.")

